{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a2ac28-05ee-48ec-b0d7-051decd0160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "258ba448-dc9f-4afc-9a65-f8a80d19da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z : np.ndarray) -> np.ndarray:\n",
    "    a = 1/(1+np.exp(-Z))\n",
    "    return a\n",
    "\n",
    "def relu(Z : np.ndarray) -> np.array:\n",
    "    a = np.maximum(0,Z)\n",
    "    return a\n",
    "\n",
    "def tanh(Z : np.ndarray) -> np.ndarray:\n",
    "    a = np.tanh(Z)\n",
    "    return a\n",
    "\n",
    "def softmax(Z : np.ndarray) -> np.ndarray:\n",
    "    Z_shifted = Z - np.max(Z, axis=0, keepdims=True)\n",
    "    exp_Z = np.exp(Z_shifted)\n",
    "    softmax_output = exp_Z / np.sum(exp_Z, axis=0, keepdims=True)\n",
    "    return softmax_output\n",
    "\n",
    "def d_sigmoid(Z : np.ndarray) -> np.ndarray:\n",
    "    d_g = np.multiply(sigmoid(Z),(1-sigmoid(Z)))\n",
    "    return d_g\n",
    "\n",
    "def d_relu(Z : np.ndarray) -> np.ndarray:\n",
    "    d_g = Z[Z<=0] = 0\n",
    "    return d_g\n",
    "\n",
    "def d_tanh(Z : np.ndarray) -> np.ndarray:\n",
    "    d_g = 1-np.power(tanh(Z))\n",
    "    return d_g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58639870-07b2-404c-9a30-ab1603a27eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims : list[int]) -> dict[str,np.ndarray]:\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    for l in range(1,L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb262e7-9666-4208-bc60-4d866c7b9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_layer_computation(W : np.ndarray,b : np.ndarray, A_prev : np.ndarray, activation : str)-> (np.ndarray, (np.ndarray,np.ndarray,np.ndarray)):\n",
    "    l = W.shape[0]\n",
    "    m = W.shape[1]\n",
    "\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    if activation == \"tanh\":\n",
    "        A = tanh(Z)\n",
    "    if activation == \"softmax\":\n",
    "        A = softmax(Z)\n",
    "\n",
    "    linear_cache = (W,b,A_prev)\n",
    "    Activation_cache = Z\n",
    "\n",
    "    cache = (linear_cache,Activation_cache)\n",
    "    \n",
    "    return A,cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ab5161-2bb1-4bab-823e-a673015b2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(parameters : dict[str,np.ndarray], X : np.ndarray) -> (np.ndarray,((dict[str,np.ndarray]),np.ndarray)):\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    for l in range(1,L):\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        \n",
    "        A,cache = fwd_layer_computation(W,b,A,\"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    W = parameters[f\"W{L}\"]\n",
    "    b = parameters[f\"b{L}\"]\n",
    "\n",
    "    AL,cache = fwd_layer_computation(W,b,A,\"softmax\")\n",
    "    epsilon = 1e-15\n",
    "    AL = np.clip(AL, epsilon, 1 - epsilon)\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL,caches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebdf2fed-a85d-455b-97a8-dc4e72b49f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL : np.ndarray,Y : np.ndarray) -> float:\n",
    "    m = Y.shape[1]\n",
    "    log_probs = -np.log(AL[Y,np.arange(m)])\n",
    "    J = np.sum(log_probs)/m\n",
    "\n",
    "    J = np.squeeze(J)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af9241f9-d63d-429a-8f85-2fe1aa9e47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkwd_layer_propogation(AL : np.ndarray, Y : np.ndarray, dA : np.ndarray, Z : np.ndarray, W : np.ndarray, A_prev : np.ndarray, activation : str) -> (dict[str,np.ndarray], np.ndarray):\n",
    "    grad = {}\n",
    "    m = dA.shape[1]\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = np.multiply(dA,d_sigmoid(Z))\n",
    "    if activation == \"relu\":\n",
    "        dZ = np.multiply(dA,d_relu(Z))\n",
    "    if activation == \"tanh\":\n",
    "        dZ = np.multiply(dA,d_tanh(Z))\n",
    "    if activation == \"softmax\":\n",
    "        dZ = AL-Y\n",
    "\n",
    "    dW = 1/m * np.dot(dZ,A_prev.T)\n",
    "    db = 1/m * np.sum(dZ,axis=1,keepdims = True)\n",
    "    dA = np.dot(W.T,dZ)\n",
    "    grad[\"dW\"] = dW\n",
    "    grad[\"db\"] = db\n",
    "\n",
    "    return grad, dA    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ebd66e0-775c-4877-bb45-8c69fbac9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogation(caches, Y, AL):\n",
    "    # print(f\"W1 : {caches[0][0][0].shape}\")\n",
    "    # print(f\"b1 : {caches[0][0][1].shape}\")\n",
    "    # print(f\"A0 : {caches[0][0][2].shape}\")\n",
    "    # print(f\"Z1 : {caches[0][1].shape}\")\n",
    "    # print(f\"W2 : {caches[1][0][0].shape}\")\n",
    "    # print(f\"b2 : {caches[1][0][1].shape}\")\n",
    "    # print(f\"A1 : {caches[1][0][2].shape}\")\n",
    "    # print(f\"Z2 : {caches[1][1].shape}\")\n",
    "    # print(f\"W3 : {caches[2][0][0].shape}\")\n",
    "    # print(f\"b3 : {caches[2][0][1].shape}\")\n",
    "    # print(f\"A2 : {caches[2][0][2].shape}\")\n",
    "    # print(f\"Z3 : {caches[2][1].shape}\")\n",
    "    L = len(caches)\n",
    "    linear_cache = caches[L-1][0]\n",
    "    activation_cache = caches[L-1][1]\n",
    "    Z = activation_cache\n",
    "    W = linear_cache[0]\n",
    "    b = linear_cache[1]\n",
    "    A_prev = linear_cache[2]\n",
    "    \n",
    "    dA = - Y / AL\n",
    "    grads = {}\n",
    "    \n",
    "    grad,dA = bkwd_layer_propogation(AL,Y,dA,Z,W,A_prev,\"softmax\")\n",
    "    grads[f\"dW{L}\"] = grad[\"dW\"]\n",
    "    grads[f\"db{L}\"] = grad[\"db\"]\n",
    "    for l in range(L-2,-1,-1):\n",
    "        linear_cache = caches[l][0]\n",
    "        activation_cache = caches[l][1]\n",
    "\n",
    "        Z = activation_cache\n",
    "        W = linear_cache[0]\n",
    "        b = linear_cache[1]\n",
    "        A_prev = linear_cache[2]\n",
    "\n",
    "        grad,dA = bkwd_layer_propogation(AL,Y,dA,Z,W,A_prev,\"relu\")\n",
    "        grads[f\"dW{l+1}\"] = grad[\"dW\"]\n",
    "        grads[f\"db{l+1}\"] = grad[\"db\"]\n",
    "\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14b2f6b7-a927-4ea8-99f0-50a786f4c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        m = W.shape[0]\n",
    "        # print(f\"layer {l}\")\n",
    "        # print(f\"W : {W.shape}\")\n",
    "        # print(f\"b : {b.shape}\")\n",
    "        \n",
    "        dW = grads[f\"dW{l}\"]\n",
    "        db = grads[f\"db{l}\"]\n",
    "        \n",
    "        W = W - learning_rate*dW\n",
    "        b = b - learning_rate*db\n",
    "\n",
    "        parameters[f\"W{l}\"] = W\n",
    "        parameters[f\"b{l}\"] = b        \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23fc405-c784-4897-8b04-83696523cdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from load_MNIST import MnistDataloader\n",
    "from os.path import join\n",
    "%matplotlib inline\n",
    "\n",
    "input_path = 'datasets/MNIST_dataset/'\n",
    "training_images_filepath = join(input_path, 'train-images-idx3-ubyte/train-images-idx3-ubyte')\n",
    "training_labels_filepath = join(input_path, 'train-labels-idx1-ubyte/train-labels-idx1-ubyte')\n",
    "test_images_filepath = join(input_path, 't10k-images-idx3-ubyte/t10k-images-idx3-ubyte')\n",
    "test_labels_filepath = join(input_path, 't10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#\n",
    "# Helper function to show a list of images with their relating titles\n",
    "#\n",
    "def show_images(images, title_texts):\n",
    "    cols = 5\n",
    "    rows = int(len(images)/cols) + 1\n",
    "    plt.figure(figsize=(30,20))\n",
    "    index = 1    \n",
    "    for x in zip(images, title_texts):        \n",
    "        image = x[0]        \n",
    "        title_text = x[1]\n",
    "        plt.subplot(rows, cols, index)        \n",
    "        plt.imshow(image, cmap=plt.cm.gray)\n",
    "        if (title_text != ''):\n",
    "            plt.title(title_text, fontsize = 15);        \n",
    "        index += 1\n",
    "\n",
    "#\n",
    "# Load MINST dataset\n",
    "#\n",
    "mnist_dataloader = MnistDataloader(training_images_filepath, training_labels_filepath, test_images_filepath, test_labels_filepath)\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()\n",
    "\n",
    "#\n",
    "# Show some random training and test images \n",
    "#\n",
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append('training image [' + str(r) + '] = ' + str(y_train[r]))    \n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])        \n",
    "    titles_2_show.append('test image [' + str(r) + '] = ' + str(y_test[r]))    \n",
    "\n",
    "# show_images(images_2_show, titles_2_show)\n",
    "\n",
    "m_train = len(x_train)\n",
    "m_test = len(x_test)\n",
    "num_pixel = len(x_train[0])\n",
    "\n",
    "# print(f\"Number of training examples in training set : {m_train}\")\n",
    "# print(f\"Number of training examples in test set : {m_test}\")\n",
    "# print(f\"size of each image : {num_pixel}*{num_pixel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91f70387-ae92-4569-b1ad-92f3dd80b7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unrolling the training data\n",
    "for i in range(m_train):\n",
    "    x_train[i] = np.array(x_train[i]).reshape((-1,1))\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_train = x_train.reshape((num_pixel*num_pixel,m_train))\n",
    "\n",
    "#unrolling the test data\n",
    "for i in range(m_test):\n",
    "    x_test[i] = np.array(x_test[i]).reshape((-1,1))\n",
    "\n",
    "x_test = np.array(x_test)\n",
    "x_test = x_test.reshape((num_pixel*num_pixel,m_test))\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "y_train = y_train.reshape((60000,1)).T\n",
    "y_test = y_test.reshape((10000,1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72a1b75-c62d-4811-8ce5-4e48ddbcbd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(layer_dims, learning_rate, X, Y, num_iterations):\n",
    "\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    for i in range(num_iterations):\n",
    "        AL,caches = forward_propogation(parameters,X)\n",
    "        J = compute_cost(AL, Y)\n",
    "        grads = backpropogation(caches,Y,AL)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i%100==0:\n",
    "            print(f\"Cost over iteration {i} : {J}\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c715f218-3ccd-4e99-9e3d-276f7924a688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost over iteration 0 : 2.302724968249625\n",
      "Cost over iteration 100 : 2.302636857696377\n",
      "Cost over iteration 200 : 2.302616364768769\n",
      "Cost over iteration 300 : 2.3026097805677215\n",
      "Cost over iteration 400 : 2.302606290421627\n",
      "Cost over iteration 500 : 2.302603812846365\n",
      "Cost over iteration 600 : 2.3026018327885076\n",
      "Cost over iteration 700 : 2.302600182404442\n",
      "Cost over iteration 800 : 2.3025987819277027\n",
      "Cost over iteration 900 : 2.3025975800591056\n",
      "Cost over iteration 1000 : 2.3025965389949756\n"
     ]
    }
   ],
   "source": [
    "layer_dims = [x_train.shape[0],25,15,10]\n",
    "parameters = model_train(layer_dims, 0.3, x_train, y_train,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd17bd95-9757-4e4a-820e-7dcfd8056eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X : np.ndarray, Y : np.ndarray, parameters : dict[str,np.ndarray]):\n",
    "    m = X.shape[1]\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    A_prev = X\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "\n",
    "        Z = np.dot(W,A_prev) + b\n",
    "        A_prev = relu(Z)\n",
    "\n",
    "    W = parameters[f\"W{L}\"]\n",
    "    b = parameters[f\"b{L}\"]\n",
    "\n",
    "    Z = np.dot(W,A_prev) + b\n",
    "    AL = softmax(Z)\n",
    "\n",
    "    predictions = np.argmax(AL, axis=0)       \n",
    "    accuracy = np.mean(predictions == Y.flatten())\n",
    "    print(f\"predictions : {predictions[:10]}\")\n",
    "    print(f\"y_train : {Y[:10]}\")\n",
    "    return accuracy*100       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7a27d59-65e5-4250-9156-c676dab0c6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions : [7 4 3 7 7 1 1 1 1 4]\n",
      "y_train : [[5 0 4 ... 5 6 8]]\n",
      "Accuracy for Training set : 10.12\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for Training set : {predict(x_train,y_train,parameters)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5be88d-c52a-4c4c-8bb0-8ce8d1db511e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
