{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24a2ac28-05ee-48ec-b0d7-051decd0160b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "258ba448-dc9f-4afc-9a65-f8a80d19da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z : np.ndarray) -> np.ndarray:\n",
    "    a = 1/(1+np.exp(-z))\n",
    "    return a\n",
    "\n",
    "def relu(Z : np.ndarray) -> np.array:\n",
    "    a = np.maximum(0,Z)\n",
    "    return a\n",
    "\n",
    "def tanh(Z : np.ndarray) -> np.ndarray:\n",
    "    a = np.tanh(Z)\n",
    "    return a\n",
    "\n",
    "def d_sigmoid(Z : np.ndarray) -> np.ndarray:\n",
    "    d_g = sigmoid(Z)(1-sigmoid(Z))\n",
    "    return d_g\n",
    "\n",
    "def d_relu(Z : np.ndarray) -> np.ndarray:\n",
    "    d_g = Z[Z<=0] = 0\n",
    "    return d_g\n",
    "\n",
    "def d_tanh(Z : np.ndarray) -> np.ndarray:\n",
    "    d_g = 1-np.power(tanh(Z))\n",
    "    return d_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "58639870-07b2-404c-9a30-ab1603a27eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims : list[int]) -> dict[str,np.ndarray]:\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "\n",
    "    for l in range(1,L):\n",
    "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l],1))\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "deb262e7-9666-4208-bc60-4d866c7b9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_layer_computation(W : np.ndarray,b : np.ndarray, X : np.ndarray, activation : str)-> (np.ndarray, (np.ndarray,np.ndarray,np.ndarray)):\n",
    "    l = w.shape[0]\n",
    "    m = w.shape[1]\n",
    "\n",
    "    Z = np.dot(W,X) + b\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(z)\n",
    "    if activation == \"relu\":\n",
    "        A = relu(z)\n",
    "    if activation == \"tanh\":\n",
    "        A = tanh(z)\n",
    "\n",
    "    cache = (W,b,Z)\n",
    "    \n",
    "    return A,cache    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "51ab5161-2bb1-4bab-823e-a673015b2832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(parameters : dict[str,np.ndarray], X : np.ndarray) -> (np.ndarray,((dict[str,np.ndarray]),np.ndarray)):\n",
    "    L = len(parameters)//2\n",
    "    A = X\n",
    "    caches = []\n",
    "    for l in range(1,L):\n",
    "        W = parameters[f\"W{i}\"]\n",
    "        b = parameters[f\"b{i}\"]\n",
    "        \n",
    "        A,cache = fwd_layer_computation(W,b,A,\"relu\")\n",
    "        caches.append((cache,A))\n",
    "\n",
    "    AL,cache = fwd_layer_computation(W,b,A,\"sigmoid\")\n",
    "    caches.append((cache,AL))\n",
    "\n",
    "    return AL,caches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ebdf2fed-a85d-455b-97a8-dc4e72b49f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL : np.ndarray,Y : np.ndarray) -> float:\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    J = -(1/m)*np.sum(np.dot(Y,np.log(AL)) - (np.dot(1-Y,np.log(1-AL))), axis = 0)\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "af9241f9-d63d-429a-8f85-2fe1aa9e47dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkwd_layer_propogation(dA : np.ndarray, Z : np.ndarray, W : np.ndarray, A : np.ndarray, activation : str) -> (dict[str,np.ndarray], np.ndarray):\n",
    "    grad = {}\n",
    "\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = np.multiply(dA,d_sigmoid(Z))\n",
    "    if activation == \"relu\":\n",
    "        dZ = np.multiply(dA,d_relu(Z))\n",
    "    if activation == \"tanh\":\n",
    "        dZ = np.multiply(dA,d_tanh(Z))\n",
    "\n",
    "    dW = np.dot(dZ,A.T)\n",
    "    db = np.sum(dZ,axis=1)\n",
    "    dA = np.dot(dW.T,dZ)\n",
    "\n",
    "    grad[\"dW\"] = dW\n",
    "    grad[\"db\"] = db\n",
    "\n",
    "    return grad, dA    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4ebd66e0-775c-4877-bb45-8c69fbac9eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogation(caches, Y):\n",
    "    L = len(caches)\n",
    "\n",
    "    linear_cache = caches[L-1][0]\n",
    "    activation_cache = caches[L-1][1]\n",
    "\n",
    "    AL = activation_cache\n",
    "    W = linear_cache[0]\n",
    "    b = linear_cache[1]\n",
    "    Z = linear_cache[2]\n",
    "    \n",
    "    dA = -np.divide(Y,AL) + np.divide((1-Y),(1-AL))\n",
    "    grads = {}\n",
    "\n",
    "    grad,dA = bkwd_layer_propogation(dA,Z,W,AL,\"sigmoid\")\n",
    "    grads[f\"dW{L}\"] = grad[\"dW\"]\n",
    "    grads[f\"db{L}\"] = grad[\"db\"]\n",
    "\n",
    "    for l in range(L-2,0,-1):\n",
    "        linear_cache = caches[l][0]\n",
    "        activation_cache = caches[l][1]\n",
    "\n",
    "        AL = activation_cache\n",
    "        W = linear_cache[0]\n",
    "        b = linear_cache[1]\n",
    "        Z = linear_cache[2]\n",
    "\n",
    "        grad,dA = bkwd_layer_propogation(dA,Z,W,AL,\"relu\")\n",
    "        grads[f\"dW{l}\"] = grad[\"dW\"]\n",
    "        grads[f\"db{l}\"] = grad[\"db\"]\n",
    "\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "14b2f6b7-a927-4ea8-99f0-50a786f4c19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters)/2\n",
    "\n",
    "    for l in range(1,L+1):\n",
    "        W = parameters[f\"W{l}\"]\n",
    "        b = parameters[f\"b{l}\"]\n",
    "        m = W.shape[0]\n",
    "\n",
    "        dW = grads[f\"dW{l}\"]\n",
    "        db = grads[f\"db{l}\"]\n",
    "        \n",
    "        W = W - (1/m)*np.sum(dW,axis=1)\n",
    "        b = b - (1/m)*np.sum(db,axis=1)\n",
    "\n",
    "        parameters[f\"W{l}\"] = W\n",
    "        parameters[f\"W{l}\"] = b\n",
    "\n",
    "    return parameters\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fc405-c784-4897-8b04-83696523cdec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
